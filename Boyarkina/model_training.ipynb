{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Подключим необходимые для работы библиотеки. Будем использовать word2vec преобразование слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-26 23:05:57.486174: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import joblib\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "import re\n",
    "from sklearn import linear_model, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sqlite3\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Напишем функцию для токенизации и лемматизации слов, а также удаления стопслов из документа (поста, описания группы и любого другого текстового параметра). Функция токенизирует документ, если в нем содержалось более 2 значащих слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "patterns = \"[«»!#$%&'()*+,./:;<=>?@[\\]^_`{|}~—\\\"\\-]+\"\n",
    "stopwords_ru_en = stopwords.words(\"russian\") + stopwords.words(\"english\")\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "def lemmatize(doc):\n",
    "    doc = doc.lower()\n",
    "    doc = re.sub(patterns, ' ', doc)\n",
    "    tokens = []\n",
    "    for token in doc.split():\n",
    "        token = token.strip()\n",
    "        if token and token not in stopwords_ru_en:\n",
    "            token = morph.normal_forms(token)[0]\n",
    "            tokens.append(token)\n",
    "    if len(tokens) > 2:\n",
    "        return tokens\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Извлечем тексты всех постов из датасета, описания групп, их названия и статусы для обучения модели word2vec, разделим данные на обучающий и тестовый датасеты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "db = sqlite3.connect('groups.db')  # путь к датасету\n",
    "cursor_obj = db.cursor()\n",
    "cursor_obj.execute(\"select * from groups\")\n",
    "rows = cursor_obj.fetchall()\n",
    "db.close()\n",
    "\n",
    "train_data, test_data = train_test_split(rows, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем все тексты, которые содержатся в нашем датасете, в единый список документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_texts = []\n",
    "for group in rows:\n",
    "    name = lemmatize(group[1])\n",
    "    if name is not None:\n",
    "        all_texts.append(name)\n",
    "    status = lemmatize(group[2])\n",
    "    if status is not None:\n",
    "        all_texts.append(status)\n",
    "    description = lemmatize(group[3])\n",
    "    if description is not None:\n",
    "        all_texts.append(description)\n",
    "    posts = json.loads(group[12])\n",
    "    for post in posts:\n",
    "        prep_post = lemmatize(post['text'])\n",
    "        if prep_post is not None:\n",
    "            all_texts.append(prep_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим w2v модель на наших текстах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2757601, 10085300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model = Word2Vec(\n",
    "    all_texts,\n",
    "    min_count=10,\n",
    "    window=2,\n",
    "    vector_size=300,\n",
    "    negative=10,\n",
    "    alpha=0.03,\n",
    "    min_alpha=0.0007,\n",
    "    sample=6e-5,\n",
    "    sg=1)\n",
    "\n",
    "w2v_model.train(all_texts, total_examples=w2v_model.corpus_count, epochs=100, report_delay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Запишем вспомогательные функции для обработки текстов. Первая вычисляет сходство текста с какой-либо тематикой. Вторая функция вычисляет сходство текста со всеми тематиками и формирует вектор для текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_text_similarity(tokenized_text, theme_word):\n",
    "    text_similarity = 0.\n",
    "    used_words = 0\n",
    "    for word in tokenized_text:\n",
    "        if word in w2v_model.wv.key_to_index:\n",
    "            text_similarity += w2v_model.wv.similarity(word, theme_word)\n",
    "            used_words += 1\n",
    "    if used_words != 0:\n",
    "        return text_similarity / used_words\n",
    "    return 0.\n",
    "\n",
    "def get_themes_characteristics(tokenized_text):\n",
    "    return [\n",
    "        get_text_similarity(tokenized_text, 'музыка'),\n",
    "        get_text_similarity(tokenized_text, 'путешествие'),\n",
    "        get_text_similarity(tokenized_text, 'программирование'),\n",
    "        get_text_similarity(tokenized_text, 'мем')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим функцию для преобразования информации о группе в вектор. В векторе содержатся данные о названии группы, статусе и описании, численные значения о группе (число подписчиков и т.п.), а также данные о 15 постах группы (текст поста, число лайков, репостов, прикрепленных фото и т.п.). Также написана функция для получения только текстовых признаков группы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorized_group_info(group_info):\n",
    "    group_vector = []\n",
    "    if lemmatize(group_info[1]) is not None:\n",
    "        group_vector.extend(get_themes_characteristics(lemmatize(group_info[1])))\n",
    "    else:\n",
    "        group_vector.extend([0.] * 4)\n",
    "    if lemmatize(group_info[2]) is not None:\n",
    "        group_vector.extend(get_themes_characteristics(lemmatize(group_info[2])))\n",
    "    else:\n",
    "        group_vector.extend([0.] * 4)\n",
    "    if lemmatize(group_info[3]) is not None:\n",
    "        group_vector.extend(get_themes_characteristics(lemmatize(group_info[3])))\n",
    "    else:\n",
    "        group_vector.extend([0.] * 4)\n",
    "    group_vector.extend(group_info[6:11])\n",
    "    posts = json.loads(group_info[12])\n",
    "    for post in posts:\n",
    "        if lemmatize(post['text']) is not None:\n",
    "            group_vector.extend(get_themes_characteristics(lemmatize(post['text'])))\n",
    "        else:\n",
    "            group_vector.extend([0.] * 4)\n",
    "        group_vector.extend([post['likes'], post['reposts'], post['photos_number'], post['music_number'], \n",
    "                        post['video_number'], post['links_number'], post['docs_number']])\n",
    "    while len(group_vector) < 3 * 4 + 5 + 15 * 11:\n",
    "        group_vector.extend([0.] * 11)\n",
    "    \n",
    "    return group_vector\n",
    "\n",
    "\n",
    "def get_vectorized_group_info_only_texts(group_info):\n",
    "    group_vector = []\n",
    "    if lemmatize(group_info[1]) is not None:\n",
    "        group_vector.extend(get_themes_characteristics(lemmatize(group_info[1])))\n",
    "    else:\n",
    "        group_vector.extend([0.] * 4)\n",
    "    if lemmatize(group_info[2]) is not None:\n",
    "        group_vector.extend(get_themes_characteristics(lemmatize(group_info[2])))\n",
    "    else:\n",
    "        group_vector.extend([0.] * 4)\n",
    "    if lemmatize(group_info[3]) is not None:\n",
    "        group_vector.extend(get_themes_characteristics(lemmatize(group_info[3])))\n",
    "    else:\n",
    "        group_vector.extend([0.] * 4)\n",
    "    posts = json.loads(group_info[12])\n",
    "    for post in posts:\n",
    "        if lemmatize(post['text']) is not None:\n",
    "            group_vector.extend(get_themes_characteristics(lemmatize(post['text'])))\n",
    "        else:\n",
    "            group_vector.extend([0.] * 4)\n",
    "    while len(group_vector) < 3 * 4 + 15 * 4:\n",
    "        group_vector.extend([0.] * 4)\n",
    "    \n",
    "    return group_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем векторы входных и выходных данных для моделей, используя написанные функции. Создадим следующие наборы: датасет со всеми возможными входными данными, датасет без информации о постах и датасет, содержащий только информацию о текстовых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes = {\n",
    "    'Музыка': 1,\n",
    "    'Путешествия': 2,\n",
    "    'Программирование': 3,\n",
    "    'Мемы': 4,\n",
    "    'Другое': 0\n",
    "}\n",
    "themes_inverse = {\n",
    "    1: 'Музыка',\n",
    "    2: 'Путешествия',\n",
    "    3: 'Программирование',\n",
    "    4: 'Мемы',\n",
    "    0: 'Другое'\n",
    "}\n",
    "\n",
    "train_input_full = []\n",
    "train_input_no_posts = []\n",
    "train_input_only_texts = []\n",
    "train_output = []\n",
    "for element in train_data:\n",
    "    train_input_full.append(get_vectorized_group_info(element))\n",
    "    train_input_no_posts.append(train_input_full[-1][:17])\n",
    "    train_input_only_texts.append(get_vectorized_group_info_only_texts(element))\n",
    "    train_output.append(themes[element[13]])\n",
    "    \n",
    "test_input_full = []\n",
    "test_input_no_posts = []\n",
    "test_input_only_texts = []\n",
    "test_output = []\n",
    "for element in test_data:\n",
    "    test_input_full.append(get_vectorized_group_info(element))\n",
    "    test_input_no_posts.append(test_input_full[-1][:17])\n",
    "    test_input_only_texts.append(get_vectorized_group_info_only_texts(element))\n",
    "    test_output.append(themes[element[13]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим классификатор RidgeClassifier на полных данных и посчитаем метрики для оценки модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.15      0.22        13\n",
      "           1       0.58      1.00      0.74         7\n",
      "           2       0.50      0.62      0.56         8\n",
      "           3       0.58      0.64      0.61        11\n",
      "           4       0.36      0.36      0.36        11\n",
      "\n",
      "    accuracy                           0.50        50\n",
      "   macro avg       0.49      0.56      0.50        50\n",
      "weighted avg       0.47      0.50      0.46        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_ridge_full = linear_model.RidgeClassifier()\n",
    "model_ridge_full.fit(train_input_full, train_output)\n",
    "\n",
    "predicted_test_ridge = model_ridge_full.predict(test_input_full)\n",
    "print(metrics.classification_report(predicted_test_ridge, test_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для данных, не использующих информацию о постах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.10      0.13        10\n",
      "           1       0.75      0.64      0.69        14\n",
      "           2       0.90      0.82      0.86        11\n",
      "           3       0.67      1.00      0.80         8\n",
      "           4       0.36      0.57      0.44         7\n",
      "\n",
      "    accuracy                           0.62        50\n",
      "   macro avg       0.58      0.63      0.59        50\n",
      "weighted avg       0.61      0.62      0.60        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_ridge_no_posts = linear_model.RidgeClassifier()\n",
    "model_ridge_no_posts.fit(train_input_no_posts, train_output)\n",
    "\n",
    "predicted_test_no_posts = model_ridge_no_posts.predict(test_input_no_posts)\n",
    "print(metrics.classification_report(predicted_test_no_posts, test_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для данных, использующих только текстовую информацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.25      0.31         8\n",
      "           1       0.58      0.78      0.67         9\n",
      "           2       0.90      1.00      0.95         9\n",
      "           3       0.92      0.92      0.92        12\n",
      "           4       0.64      0.58      0.61        12\n",
      "\n",
      "    accuracy                           0.72        50\n",
      "   macro avg       0.69      0.71      0.69        50\n",
      "weighted avg       0.70      0.72      0.71        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_ridge_only_texts = linear_model.RidgeClassifier()\n",
    "model_ridge_only_texts.fit(train_input_only_texts, train_output)\n",
    "\n",
    "predicted_test_only_texts = model_ridge_only_texts.predict(test_input_only_texts)\n",
    "print(metrics.classification_report(predicted_test_only_texts, test_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модели многослойного перцептрона на наших данных, чтобы сравнить результат ее работы с RidgeClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.75      0.26      0.39        34\n",
      "           2       0.20      0.25      0.22         8\n",
      "           3       0.33      0.80      0.47         5\n",
      "           4       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.30        50\n",
      "   macro avg       0.26      0.26      0.22        50\n",
      "weighted avg       0.58      0.30      0.35        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np_train_input_full = np.array(train_input_full)\n",
    "np_test_input_full = np.array(test_input_full)\n",
    "\n",
    "train_output_onehot = keras.utils.to_categorical(np.array(train_output))\n",
    "\n",
    "model_seq_full = keras.Sequential([\n",
    "        keras.layers.Dense(np_train_input_full.shape[1], activation='elu'),\n",
    "        keras.layers.Dense(100, activation='elu'),\n",
    "        keras.layers.Dense(60, activation='elu'),\n",
    "        keras.layers.Dense(train_output_onehot.shape[1], activation='softmax')\n",
    "    ])\n",
    "model_seq_full.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "history_seq_full = model_seq_full.fit(np_train_input_full, train_output_onehot, epochs=100, verbose=0)\n",
    "\n",
    "predicted_seq_test_full = model_seq_full.predict(np_test_input_full)\n",
    "predicted_res_test_full = []\n",
    "for elem in predicted_seq_test_full:\n",
    "    max_index = 0\n",
    "    for i in range(5):\n",
    "        if elem[i] > elem[max_index]:\n",
    "            max_index = i\n",
    "    predicted_res_test_full.append(max_index)\n",
    "print(metrics.classification_report(predicted_res_test_full, test_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.12      0.21        33\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.67      0.47      0.55        17\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.24        50\n",
      "   macro avg       0.29      0.12      0.15        50\n",
      "weighted avg       0.75      0.24      0.33        50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "np_train_input_no_posts = np.array(train_input_no_posts)\n",
    "np_test_input_no_posts = np.array(test_input_no_posts)\n",
    "\n",
    "model_seq_no_posts = keras.Sequential([\n",
    "        keras.layers.Dense(np_train_input_no_posts.shape[1], activation='elu'),\n",
    "        keras.layers.Dense(100, activation='elu'),\n",
    "        keras.layers.Dense(60, activation='elu'),\n",
    "        keras.layers.Dense(train_output_onehot.shape[1], activation='softmax')\n",
    "    ])\n",
    "model_seq_no_posts.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "history_seq_no_posts = model_seq_no_posts.fit(np_train_input_no_posts, train_output_onehot, epochs=100, verbose=0)\n",
    "\n",
    "predicted_seq_test_no_posts = model_seq_no_posts.predict(np_test_input_no_posts)\n",
    "predicted_res_test_no_posts = []\n",
    "for elem in predicted_seq_test_no_posts:\n",
    "    max_index = 0\n",
    "    for i in range(5):\n",
    "        if elem[i] > elem[max_index]:\n",
    "            max_index = i\n",
    "    predicted_res_test_no_posts.append(max_index)\n",
    "print(metrics.classification_report(predicted_res_test_no_posts, test_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.25      0.35        12\n",
      "           1       0.67      0.80      0.73        10\n",
      "           2       0.90      0.90      0.90        10\n",
      "           3       0.83      0.91      0.87        11\n",
      "           4       0.55      0.86      0.67         7\n",
      "\n",
      "    accuracy                           0.72        50\n",
      "   macro avg       0.71      0.74      0.70        50\n",
      "weighted avg       0.72      0.72      0.69        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np_train_input_only_texts = np.array(train_input_only_texts)\n",
    "np_test_input_only_texts = np.array(test_input_only_texts)\n",
    "\n",
    "model_seq_only_texts = keras.Sequential([\n",
    "        keras.layers.Dense(np_train_input_only_texts.shape[1], activation='elu'),\n",
    "        keras.layers.Dense(100, activation='elu'),\n",
    "        keras.layers.Dense(60, activation='elu'),\n",
    "        keras.layers.Dense(train_output_onehot.shape[1], activation='softmax')\n",
    "    ])\n",
    "model_seq_only_texts.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "history_seq_only_texts = model_seq_only_texts.fit(np_train_input_only_texts, train_output_onehot, epochs=100, verbose=0)\n",
    "\n",
    "predicted_seq_test_only_texts = model_seq_only_texts.predict(np_test_input_only_texts)\n",
    "predicted_res_test_only_texts = []\n",
    "for elem in predicted_seq_test_only_texts:\n",
    "    max_index = 0\n",
    "    for i in range(5):\n",
    "        if elem[i] > elem[max_index]:\n",
    "            max_index = i\n",
    "    predicted_res_test_only_texts.append(max_index)\n",
    "print(metrics.classification_report(predicted_res_test_only_texts, test_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно сделать вывод, что в первых двух моделях слишком много данных, к тому же они плохо нормированы (характеристики текстов меньше 1, в то время как число лайков/комментариев имеет значения в тысячи, а то и сотни тысяч). Третий вариант с использованием только текстовых данных наиболее работающий, имеет неплохие показатели в обоих случаях. Многослойный перцептрон работает лучше обычного линейного классификатора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Сохраним обученные модели для дальнейшего использования:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model_ridge_full, 'model_ridge_full.pkl')\n",
    "joblib.dump(model_ridge_no_posts, 'model_ridge_no_posts.pkl')\n",
    "joblib.dump(model_ridge_only_texts, 'model_ridge_only_texts.pkl')\n",
    "model_seq_full.save('model_seq_full.h5')\n",
    "model_seq_no_posts.save('model_seq_no_posts.h5')\n",
    "model_seq_only_texts.save('model_seq_only_texts.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}